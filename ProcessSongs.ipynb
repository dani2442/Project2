{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "149b37f1-c0cf-49bb-b593-937531d0ef9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import os\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "3865f104-b16c-4fd4-8049-99b97ca68ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS = ('training', 'test', 'validation')\n",
    "PERCENTAGES = (0.7, 0.15, 0.15)\n",
    "f_split = lambda : random.choices(SPLITS, weights=PERCENTAGES)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "75e5dd1e-0687-4a5b-ac11-1a2da2c1d5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'data/raw/Progressive_Rock_Songs/'\n",
    "progressive_rock = [[os.path.join(dirpath,f), 1] for (dirpath, dirnames, filenames) in os.walk(mypath) for f in filenames if f.endswith('.mp3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "dad8fb59-316d-4a44-8120-36002dcc4e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "mypath = 'data/raw/Not_Progressive_Rock/'\n",
    "not_progressive_rock = [[os.path.join(dirpath,f), 0] for (dirpath, dirnames, filenames) in os.walk(mypath) for f in filenames if f.endswith('.mp3')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "244efc86-9df7-4a03-b64c-4ee64a0ef9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = not_progressive_rock + progressive_rock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "4f582a02-ca0c-43ab-8a18-c9d1423b25c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_df = pd.DataFrame(dataset)\n",
    "my_df.to_csv('annotations_v2.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9187fecc-2ecd-47bd-9bea-cbbef34d990f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/raw/Not_Progressive_Rock/Other_Songs\\01 -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/raw/Not_Progressive_Rock/Other_Songs\\01 -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/raw/Not_Progressive_Rock/Other_Songs\\01 -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/raw/Not_Progressive_Rock/Other_Songs\\01 -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/raw/Not_Progressive_Rock/Other_Songs\\01 -...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>data/raw/Progressive_Rock_Songs/L'evoluzione.mp3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>data/raw/Progressive_Rock_Songs/MIKE OLDFIELD ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>data/raw/Progressive_Rock_Songs/Pain of Salvat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>data/raw/Progressive_Rock_Songs/The Flower Kin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>data/raw/Progressive_Rock_Songs/Thick As A Bri...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>272 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     0  1\n",
       "0    data/raw/Not_Progressive_Rock/Other_Songs\\01 -...  0\n",
       "1    data/raw/Not_Progressive_Rock/Other_Songs\\01 -...  0\n",
       "2    data/raw/Not_Progressive_Rock/Other_Songs\\01 -...  0\n",
       "3    data/raw/Not_Progressive_Rock/Other_Songs\\01 -...  0\n",
       "4    data/raw/Not_Progressive_Rock/Other_Songs\\01 -...  0\n",
       "..                                                 ... ..\n",
       "267   data/raw/Progressive_Rock_Songs/L'evoluzione.mp3  1\n",
       "268  data/raw/Progressive_Rock_Songs/MIKE OLDFIELD ...  1\n",
       "269  data/raw/Progressive_Rock_Songs/Pain of Salvat...  1\n",
       "270  data/raw/Progressive_Rock_Songs/The Flower Kin...  1\n",
       "271  data/raw/Progressive_Rock_Songs/Thick As A Bri...  1\n",
       "\n",
       "[272 rows x 2 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('annotations_v2.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe463376-3ae5-4732-8a27-5e7588a1c504",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_audio(path, sr=16000):\n",
    "    x , sr = librosa.load(path, sr=sr)\n",
    "    mel_spectrogram = librosa.feature.melspectrogram(y=x, n_fft=400, hop_length=160, n_mels=80, fmax=8000, sr=sr)\n",
    "    log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "    return log_mel_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "031a51a7-f60b-4395-80c4-ea3caa4d8c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataset(dataset, save_dir='data/process/'):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    data = []\n",
    "    \n",
    "    for i in tqdm(range(len(dataset))):\n",
    "        try:\n",
    "            path, label = dataset[i]        \n",
    "            mat = process_audio(path)\n",
    "            save_path = save_dir + f'{i}.npy'\n",
    "            np.save(save_path, mat)\n",
    "            data += [save_path,label]\n",
    "        except:\n",
    "            print(f\"Exception in song {i}\")\n",
    "    \n",
    "    with open(\"annotation.csv\", \"wb\") as f:\n",
    "        writer = csv.writer(f)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fa678df-5909-42b9-b66a-c2286f048745",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7c75015c-8d02-47e5-86ce-210e3dd40b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = process_audio(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0441bbd0-2dba-4977-ab77-285e0c2e6ee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 20618)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce47514d-043a-4657-9dfd-f5c06e7a9e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for i in range(int(len(data)/2)):\n",
    "    d+= [[data[2*i], data[2*i+1]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4cfec3a8-1b5c-4108-a1e5-0baa9846e1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3fed647d-e804-4adb-8208-d4f27d5145a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(d)\n",
    "df.to_csv('annotation.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb38fd6-3d36-4c14-9002-1474ed1076f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|████▏                                                                                                                             | 9/279 [01:02<26:47,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in song 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████                                                                                                                            | 11/279 [01:11<24:31,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in song 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█████▌                                                                                                                           | 12/279 [01:17<24:25,  5.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in song 11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████                                                                                                                           | 13/279 [01:32<36:44,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exception in song 12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|██████▍                                                                                                                          | 14/279 [01:41<38:10,  8.64s/it]"
     ]
    }
   ],
   "source": [
    "process_dataset(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d80545ed-65a0-4070-849a-edf86ee8005f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "import utils\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ac81ef8-4785-439e-b898-06b334200b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.model import ModelDimensions, AudioEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c689bf7-4c0b-4fdb-a644-47d5f40d7ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = ModelDimensions(n_mels=80, n_audio_ctx=1500, n_audio_state=384, n_audio_head=6, n_audio_layer=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5c9f253-8fb2-456c-83e0-0cf375f72d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioClassifier(nn.Module):\n",
    "    def __init__(self, dims: ModelDimensions):\n",
    "        super().__init__()\n",
    "        self.dims = dims\n",
    "        self.encoder = AudioEncoder(\n",
    "            self.dims.n_mels,\n",
    "            self.dims.n_audio_ctx,\n",
    "            self.dims.n_audio_state,\n",
    "            self.dims.n_audio_head,\n",
    "            self.dims.n_audio_layer,\n",
    "        )\n",
    "        \n",
    "    def forward(self, mel: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
    "        return self.encoder(mel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "95912568-59e8-4a91-9dc2-64c2a1ad2376",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AudioClassifier(dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffc39a8b-2130-4833-b952-3d6328fcbecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.zeros((10, 80, 1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee48c754-de83-4f37-a977-da2d97a98d87",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "incorrect audio shape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [24], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m y \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [21], line 14\u001b[0m, in \u001b[0;36mAudioClassifier.forward\u001b[1;34m(self, mel)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, mel: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\utils\\model.py:143\u001b[0m, in \u001b[0;36mAudioEncoder.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    140\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mgelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))\n\u001b[0;32m    141\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 143\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding\u001b[38;5;241m.\u001b[39mshape, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mincorrect audio shape\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    144\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpositional_embedding)\u001b[38;5;241m.\u001b[39mto(x\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n",
      "\u001b[1;31mAssertionError\u001b[0m: incorrect audio shape"
     ]
    }
   ],
   "source": [
    "y = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4977211d-4456-42c4-8c85-734114aaffe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ffmpeg\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "04c81d26-4e4f-4f21-8941-bdf31c7e9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "N_FFT = 400\n",
    "N_MELS = 80\n",
    "HOP_LENGTH = 160\n",
    "CHUNK_LENGTH = 30\n",
    "N_SAMPLES = CHUNK_LENGTH * SAMPLE_RATE  # 480000: number of samples in a chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44cd10c3-ddcb-4c62-a518-91a62a530219",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio(file: str, sr: int = SAMPLE_RATE):\n",
    "    \"\"\"\n",
    "    Open an audio file and read as mono waveform, resampling as necessary\n",
    "    Parameters\n",
    "    ----------\n",
    "    file: str\n",
    "        The audio file to open\n",
    "    sr: int\n",
    "        The sample rate to resample the audio if necessary\n",
    "    Returns\n",
    "    -------\n",
    "    A NumPy array containing the audio waveform, in float32 dtype.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # This launches a subprocess to decode audio while down-mixing and resampling as necessary.\n",
    "        # Requires the ffmpeg CLI and `ffmpeg-python` package to be installed.\n",
    "        out, _ = (\n",
    "            ffmpeg.input(file, threads=0)\n",
    "            .output(\"-\", format=\"s16le\", acodec=\"pcm_s16le\", ac=1, ar=sr)\n",
    "            .run(cmd=[\"ffmpeg\", \"-nostdin\"], capture_stdout=True, capture_stderr=True)\n",
    "        )\n",
    "    except ffmpeg.Error as e:\n",
    "        raise RuntimeError(f\"Failed to load audio: {e.stderr.decode()}\") from e\n",
    "\n",
    "    return np.frombuffer(out, np.int16).flatten().astype(np.float32) / 32768.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "251e287e-3d11-4242-aa1c-551d8e85dec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed(\n",
    "            \"assets/mel_filters.npz\",\n",
    "            mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2746af73-2cda-4f5c-8bb7-32f0ff8bf074",
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=None)\n",
    "def mel_filters(device, n_mels: int = N_MELS) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    load the mel filterbank matrix for projecting STFT into a Mel spectrogram.\n",
    "    Allows decoupling librosa dependency; saved using:\n",
    "        np.savez_compressed(\n",
    "            \"mel_filters.npz\",\n",
    "            mel_80=librosa.filters.mel(sr=16000, n_fft=400, n_mels=80),\n",
    "        )\n",
    "    \"\"\"\n",
    "    assert n_mels == 80, f\"Unsupported n_mels: {n_mels}\"\n",
    "    with np.load(os.path.join(\"assets\",\"mel_filters.npz\")) as f:\n",
    "        return torch.from_numpy(f[f\"mel_{n_mels}\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b72b40e-bfee-451f-bbe7-309f2e4fa3ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mel_spectrogram(audio, n_mels: int = N_MELS):\n",
    "    \"\"\"\n",
    "    Compute the log-Mel spectrogram of\n",
    "    Parameters\n",
    "    ----------\n",
    "    audio: Union[str, np.ndarray, torch.Tensor], shape = (*)\n",
    "        The path to audio or either a NumPy array or Tensor containing the audio waveform in 16 kHz\n",
    "    n_mels: int\n",
    "        The number of Mel-frequency filters, only 80 is supported\n",
    "    Returns\n",
    "    -------\n",
    "    torch.Tensor, shape = (80, n_frames)\n",
    "        A Tensor that contains the Mel spectrogram\n",
    "    \"\"\"\n",
    "    if not torch.is_tensor(audio):\n",
    "        if isinstance(audio, str):\n",
    "            audio = load_audio(audio)\n",
    "        audio = torch.from_numpy(audio)\n",
    "\n",
    "    window = torch.hann_window(N_FFT).to(audio.device)\n",
    "    stft = torch.stft(audio, N_FFT, HOP_LENGTH, window=window, return_complex=True)\n",
    "    magnitudes = stft[:, :-1].abs() ** 2\n",
    "\n",
    "    filters = mel_filters(audio.device, n_mels)\n",
    "    mel_spec = filters @ magnitudes\n",
    "\n",
    "    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n",
    "    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
    "    log_spec = (log_spec + 4.0) / 4.0\n",
    "    return log_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f18ef212-feda-4e6f-8d27-442d6fbdbb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = log_mel_spectrogram(dataset[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6d91d343-3e21-4e5c-ba2e-d1429e53d170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3298878,)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = load_audio(dataset[0][0])\n",
    "c.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8dc6e1be-13b3-4656-908f-268b2b432755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/raw/Not_Progressive_Rock/Other_Songs\\\\01.ArmenMiran-PreciousStory.mp3'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[8][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "668ede87-9afc-4243-be8f-70787be89d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from random import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "7a130492-e6e9-4d08-84ea-5a5fb4d5c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SongDatasetTest_v2(Dataset):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def __len__(self):\n",
    "        return 99999999\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if random()>0.1: raise StopIteration()\n",
    "        return np.array([0])  else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b15caa81-267b-4456-8566-cc8a65a04f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = SongDatasetTest_v2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b00914c9-66e6-4033-85d3-44d3344a9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fe23fcf9-7df7-434d-b9b8-a2d6b49ea7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0]], dtype=torch.int32)\n",
      "tensor([[0]], dtype=torch.int32)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m loader:\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(i)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:61\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\PycharmProjects\\Project2\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:151\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    147\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    148\u001b[0m             \u001b[38;5;66;03m# The sequence type may not support `__init__(iterable)` (e.g., `range`).\u001b[39;00m\n\u001b[0;32m    149\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]\n\u001b[1;32m--> 151\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem_type))\n",
      "\u001b[1;31mTypeError\u001b[0m: default_collate: batch must contain tensors, numpy arrays, numbers, dicts or lists; found <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "for i in loader:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df16add-480c-4cd7-92e2-4a9e12a78153",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project2_venv",
   "language": "python",
   "name": "project2_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
